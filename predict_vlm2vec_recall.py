      

import json
import torch
from PIL import Image
from tqdm import tqdm
import os
import sys
import warnings


def check_flash_attention_support():

    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„GPU
        if not torch.cuda.is_available():
            return False, "CUDAä¸å¯ç”¨"
        
        # è·å–GPUè®¡ç®—èƒ½åŠ›
        device_capability = torch.cuda.get_device_capability()
        major, minor = device_capability
        compute_capability = major * 10 + minor
        
        # Flash Attention 2éœ€è¦è®¡ç®—èƒ½åŠ› >= 8.0 (AmpereåŠä»¥ä¸Šæ¶æ„)
        # Flash Attention 1éœ€è¦è®¡ç®—èƒ½åŠ› >= 7.5 (TuringåŠä»¥ä¸Šæ¶æ„)
        if compute_capability >= 80:
            # å°è¯•å¯¼å…¥flash_attn
            try:
                import flash_attn
                return True, f"æ”¯æŒFlash Attention (GPUè®¡ç®—èƒ½åŠ›: {major}.{minor})"
            except ImportError:
                return False, f"GPUæ”¯æŒä½†æœªå®‰è£…flash_attnåŒ… (è®¡ç®—èƒ½åŠ›: {major}.{minor})"
        else:
            return False, f"GPUè®¡ç®—èƒ½åŠ›ä¸è¶³ (å½“å‰: {major}.{minor}, éœ€è¦: >= 8.0)"
            
    except Exception as e:
        return False, f"æ£€æµ‹å¤±è´¥: {str(e)}"


def configure_attention_backend():

    is_supported, message = check_flash_attention_support()
    
    print("\n" + "="*80)
    print("æ³¨æ„åŠ›æœºåˆ¶é…ç½®")
    print("="*80)
    
    if is_supported:
        print(f"âœ… {message}")
        print("   ä½¿ç”¨: Flash Attention (æœ€å¿«)")
        os.environ["ATTN_IMPLEMENTATION"] = "flash_attention_2"
        # åŒæ—¶è®¾ç½®transformersä½¿ç”¨çš„ç¯å¢ƒå˜é‡
        os.environ["USE_FLASH_ATTENTION"] = "1"
        return "flash_attn"
    else:
        print(f"âš ï¸  {message}")
        
        # æ£€æŸ¥PyTorchç‰ˆæœ¬æ˜¯å¦æ”¯æŒSDPA
        pytorch_version = torch.__version__
        major, minor = map(int, pytorch_version.split('.')[:2])
        
        if major >= 2:  # PyTorch 2.0+æ”¯æŒSDPA
            print("   é™çº§ä½¿ç”¨: Scaled Dot Product Attention (SDPA)")
            print("   æ€§èƒ½: ä¸­ç­‰ï¼Œä½†æ¯”eageræ¨¡å¼å¿«")
            os.environ["ATTN_IMPLEMENTATION"] = "sdpa"
            os.environ["USE_FLASH_ATTENTION"] = "0"
            return "sdpa"
        else:
            print("   é™çº§ä½¿ç”¨: Eager Attention (æ ‡å‡†å®ç°)")
            print("   æ€§èƒ½: è¾ƒæ…¢ï¼Œä½†å…¼å®¹æ€§æœ€å¥½")
            os.environ["ATTN_IMPLEMENTATION"] = "eager"
            os.environ["USE_FLASH_ATTENTION"] = "0"
            return "eager"
    
    print("="*80 + "\n")


_attn_type = configure_attention_backend()

# ç°åœ¨æ‰å¯¼å…¥VLM2Vecæ¨¡å—
from src.model.model import MMEBModel
from src.arguments import ModelArguments, DataArguments
from src.model.processor import load_processor, QWEN2_VL, VLM_IMAGE_TOKENS


INPUT_FILE = "/public/home/xiaojw2025/Workspace/RAHP/DATASET/VG150/test_200_images.json"
OUTPUT_FILE = "/public/home/xiaojw2025/Workspace/VLM2Vec/predict/recall_results_200.json"

# 50ä¸ªè°“è¯åˆ—è¡¨
PREDICATES = [
    "above", "across", "against", "along", "and", "at", "attached to", "behind",
    "belonging to", "between", "carrying", "covered in", "covering", "eating",
    "flying in", "for", "from", "growing on", "hanging from", "has", "holding",
    "in", "in front of", "laying on", "looking at", "lying on", "made of",
    "mounted on", "near", "of", "on", "on back of", "over", "painted on",
    "parked on", "part of", "playing", "riding", "says", "sitting on",
    "standing on", "to", "under", "using", "walking in", "walking on",
    "watching", "wearing", "wears", "with"
]


def format_bbox_as_special_token(bbox, normalize=True, original_width=1024, original_height=1024):
    """å°†è¾¹ç•Œæ¡†è½¬æ¢ä¸ºQwen2-VLçš„special tokenæ ¼å¼"""
    if isinstance(bbox, (list, tuple)) and len(bbox) == 4:
        x1, y1, x2, y2 = bbox
        
        if normalize:
            x1_norm = int((x1 / original_width) * 1000)
            y1_norm = int((y1 / original_height) * 1000)
            x2_norm = int((x2 / original_width) * 1000)
            y2_norm = int((y2 / original_height) * 1000)
            
            x1_norm = max(0, min(x1_norm, 999))
            y1_norm = max(0, min(y1_norm, 999))
            x2_norm = max(0, min(x2_norm, 999))
            y2_norm = max(0, min(y2_norm, 999))
            
            x1_norm, x2_norm = min(x1_norm, x2_norm), max(x1_norm, x2_norm)
            y1_norm, y2_norm = min(y1_norm, y2_norm), max(y1_norm, y2_norm)
            
            if x1_norm == x2_norm:
                x2_norm = min(x1_norm + 1, 999)
            if y1_norm == y2_norm:
                y2_norm = min(y1_norm + 1, 999)
            
            return f"<|box_start|>({x1_norm}, {y1_norm}), ({x2_norm}, {y2_norm})<|box_end|>"
    return ""

def format_object_with_ref(object_label):
    return f"<|object_ref_start|>{object_label}<|object_ref_end|>"


def predict_relation(model, processor, image_path, subject_obj, object_obj, original_width, original_height):
    # æ„å»ºsubjectå’Œobjectçš„ç‰¹æ®Štoken
    subj_bbox_token = format_bbox_as_special_token(
        subject_obj['bbox'], True, original_width, original_height
    )
    obj_bbox_token = format_bbox_as_special_token(
        object_obj['bbox'], True, original_width, original_height
    )
    subj_ref = format_object_with_ref(subject_obj['class_name'])
    obj_ref = format_object_with_ref(object_obj['class_name'])
    
    # æ„å»ºqueryæ–‡æœ¬ï¼ˆå›¾åƒ+ç‰©ä½“ä½ç½®ä¿¡æ¯ï¼‰
    query_text = f"{VLM_IMAGE_TOKENS[QWEN2_VL]} In the given image, the subject {subj_ref} is located at {subj_bbox_token},the object{obj_ref} is located at {obj_bbox_token}.Please return the predicate relationship between the subject and the object."
    
    # ç¼–ç queryï¼ˆå›¾åƒ+æ–‡æœ¬ï¼‰
    inputs = processor(
        text=query_text,
        images=Image.open(image_path),
        return_tensors="pt"
    )
    inputs = {key: value.to('cuda') for key, value in inputs.items()}
    inputs['pixel_values'] = inputs['pixel_values'].unsqueeze(0)
    inputs['image_grid_thw'] = inputs['image_grid_thw'].unsqueeze(0)
    
    try:
        with torch.no_grad():
            qry_output = model(qry=inputs)["qry_reps"]
    except RuntimeError as e:
        # æ•è·Flash Attentionè¿è¡Œæ—¶é”™è¯¯
        if "FlashAttention only supports Ampere" in str(e):
            raise RuntimeError(
                "æ£€æµ‹åˆ°Flash Attentionè¿è¡Œæ—¶é”™è¯¯ï¼šæ‚¨çš„GPUä¸æ”¯æŒFlash Attentionã€‚\n"
                "è¯·åœ¨è¿è¡Œè„šæœ¬å‰è®¾ç½®ç¯å¢ƒå˜é‡: export USE_FLASH_ATTENTION=0\n"
                f"åŸå§‹é”™è¯¯: {str(e)}"
            )
        else:
            raise
    
    # å¯¹50ä¸ªè°“è¯åˆ†åˆ«è®¡ç®—ç›¸ä¼¼åº¦
    predicate_scores = []
    for predicate in PREDICATES:
        inputs = processor(text=predicate, images=None, return_tensors="pt")
        inputs = {key: value.to('cuda') for key, value in inputs.items()}
        
        with torch.no_grad():
            tgt_output = model(tgt=inputs)["tgt_reps"]
            similarity = model.compute_similarity(qry_output, tgt_output)
        
        predicate_scores.append({
            'predicate': predicate,
            'similarity': similarity.item()
        })
    
    return predicate_scores


def calculate_recall_at_k_per_image(image_candidate_predictions, k=50):

    # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
    predictions_sorted = sorted(image_candidate_predictions, key=lambda x: x['similarity'], reverse=True)
    top_k_predictions = predictions_sorted[:k]
    
    # ç»Ÿè®¡top-kä¸­é¢„æµ‹æ­£ç¡®çš„å…³ç³»ï¼ˆå»é‡ï¼Œæ¯ä¸ªå…³ç³»åªç®—ä¸€æ¬¡ï¼‰
    recalled_relations = set()
    for pred in top_k_predictions:
        if pred['is_correct']:
            recalled_relations.add(pred['relation_idx'])
    
    # æ€»GTå…³ç³»æ•°ï¼ˆä»å€™é€‰ä¸­æå–å”¯ä¸€çš„relation_idxæ•°é‡ï¼‰
    total_gt_relations = len(set(pred['relation_idx'] for pred in image_candidate_predictions))
    
    recall = len(recalled_relations) / total_gt_relations if total_gt_relations > 0 else 0.0
    
    return {
        'recall@k': recall,
        'k': k,
        'recalled_relations': len(recalled_relations),
        'total_gt_relations': total_gt_relations,
        'total_candidates': len(image_candidate_predictions),
        'top_k_candidates': len(top_k_predictions)
    }


def calculate_average_recall_at_k(per_image_candidates, k=50):

    per_image_results = []
    total_recall = 0.0
    valid_images = 0
    
    for image_id, candidates in per_image_candidates.items():
        # è®¡ç®—è¯¥å›¾ç‰‡çš„recall
        img_result = calculate_recall_at_k_per_image(candidates, k)
        img_result['image_id'] = image_id
        per_image_results.append(img_result)
        
        total_recall += img_result['recall@k']
        valid_images += 1
    
    # è®¡ç®—å¹³å‡recall
    avg_recall = total_recall / valid_images if valid_images > 0 else 0.0
    
    # ç»Ÿè®¡æ€»ä½“ä¿¡æ¯
    total_gt_relations = sum(r['total_gt_relations'] for r in per_image_results)
    total_recalled_relations = sum(r['recalled_relations'] for r in per_image_results)
    
    return {
        'avg_recall@k': avg_recall,
        'k': k,
        'total_images': valid_images,
        'total_gt_relations': total_gt_relations,
        'total_recalled_relations': total_recalled_relations,
        'per_image_results': per_image_results
    }



def main():
    print("="*80)
    print("åœºæ™¯å›¾å…³ç³»é¢„æµ‹ä¸Per-Image Recall@50è®¡ç®—")
    print("="*80)

    # åŠ è½½æ•°æ®
    print(f"\nğŸ“– æ­£åœ¨åŠ è½½æ•°æ®: {INPUT_FILE}")
    with open(INPUT_FILE, 'r') as f:
        data = json.load(f)
    
    total_images = len(data)
    total_relations = sum(len(img['relations']) for img in data)
    print(f"   åŠ è½½äº† {total_images} å¼ å›¾ç‰‡ï¼Œå…± {total_relations} ä¸ªå…³ç³»")
    
    #  åŠ è½½æ¨¡å‹
    print("\nğŸ”§ æ­£åœ¨åŠ è½½VLM2Vecæ¨¡å‹...")
    

    model_args = ModelArguments(
        model_name='/public/home/xiaojw2025/Workspace/VLM2Vec/models/qwen_vl/Qwen2-VL-2B-Instruct',
        checkpoint_path='/public/home/xiaojw2025/Workspace/VLM2Vec/models/VLM2Vec-Qwen2VL-2B',
        pooling='last',
        normalize=True,
        model_backbone='qwen2_vl',
        lora=True
    )
    
    data_args = DataArguments(
        resize_min_pixels=56 * 56,
        resize_max_pixels=28 * 28 * 1280
    )
    
    processor = load_processor(model_args, data_args)
    
    # å°è¯•åŠ è½½æ¨¡å‹ï¼Œå¦‚æœflash attentionå¤±è´¥åˆ™é™çº§
    try:
        model = MMEBModel.load(model_args)
        model = model.to('cuda', dtype=torch.bfloat16)
        model.eval()
        print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    except Exception as e:
        error_msg = str(e)
        # æ£€æŸ¥æ˜¯å¦æ˜¯Flash Attentionç›¸å…³é”™è¯¯
        if ("flash" in error_msg.lower() or 
            "ampere" in error_msg.lower() or 
            "attention" in error_msg.lower() and "support" in error_msg.lower()):
            print(f"\nâš ï¸  æ¨¡å‹åŠ è½½/è¿è¡Œå¤±è´¥: {error_msg[:200]}")
            print("   æ£€æµ‹åˆ°Flash Attentionå…¼å®¹æ€§é—®é¢˜")
            print("   å°è¯•é™çº§åˆ°eageræ¨¡å¼...")
            
            # å¼ºåˆ¶ä½¿ç”¨eageræ¨¡å¼ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡ï¼‰
            os.environ["ATTN_IMPLEMENTATION"] = "eager"
            os.environ["USE_FLASH_ATTENTION"] = "0"
            
            # éœ€è¦é‡æ–°å¯¼å…¥æ¨¡å—ä»¥åº”ç”¨æ–°çš„ç¯å¢ƒå˜é‡
            import importlib
            import src.model.model
            importlib.reload(src.model.model)
            from src.model.model import MMEBModel as MMEBModelReloaded
            
            try:
                # é‡æ–°åŠ è½½å¤„ç†å™¨å’Œæ¨¡å‹
                processor = load_processor(model_args, data_args)
                model = MMEBModelReloaded.load(model_args)
                model = model.to('cuda', dtype=torch.bfloat16)
                model.eval()
                print("   âœ… æ¨¡å‹åŠ è½½å®Œæˆ (ä½¿ç”¨eageræ¨¡å¼)")
            except Exception as e2:
                print(f"\nâŒ é™çº§åä»ç„¶å¤±è´¥: {e2}")
                raise
        else:
            print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥: {error_msg}")
            raise
    
    # 3. æ‰¹é‡é¢„æµ‹
    print("\nğŸš€ å¼€å§‹æ‰¹é‡é¢„æµ‹...\n")
    
    per_image_candidates = {}  # æŒ‰å›¾ç‰‡ç»„ç»‡çš„å€™é€‰é¢„æµ‹ {image_id: [candidates]}
    all_relations_info = []  # æ¯ä¸ªå…³ç³»çš„è¯¦ç»†ä¿¡æ¯
    
    global_relation_idx = 0  # å…¨å±€å…³ç³»ç´¢å¼•
    
    for img_idx, img_data in enumerate(tqdm(data, desc="å¤„ç†å›¾ç‰‡")):
        image_id = img_data['image_id']
        image_path = img_data['image_path']
        objects = img_data['objects']
        relations = img_data['relations']
        
        # æ£€æŸ¥å›¾åƒæ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"âš ï¸  è­¦å‘Š: å›¾åƒä¸å­˜åœ¨ {image_path}")
            continue
        
        # è·å–å›¾åƒå°ºå¯¸
        with Image.open(image_path) as img:
            original_width, original_height = img.size
        
        # åˆ›å»ºç‰©ä½“IDåˆ°ç‰©ä½“ä¿¡æ¯çš„æ˜ å°„
        obj_dict = {obj['id']: obj for obj in objects}
        
        # åˆå§‹åŒ–è¯¥å›¾ç‰‡çš„å€™é€‰åˆ—è¡¨
        image_candidates = []
        image_relation_idx = 0  # è¯¥å›¾ç‰‡å†…çš„å…³ç³»ç´¢å¼•
        
        # å¯¹æ¯ä¸ªå…³ç³»è¿›è¡Œé¢„æµ‹
        for rel_idx, relation in enumerate(relations):
            subject_id = relation['subject_id']
            object_id = relation['object_id']
            gt_predicate = relation['predicate']
            
            subject_obj = obj_dict[subject_id]
            object_obj = obj_dict[object_id]
            
            # é¢„æµ‹50ä¸ªè°“è¯çš„ç›¸ä¼¼åº¦
            predicate_scores = predict_relation(
                model, processor, image_path,
                subject_obj, object_obj,
                original_width, original_height
            )
            
            # è®°å½•è¯¥å…³ç³»çš„ä¿¡æ¯
            all_relations_info.append({
                'relation_idx': global_relation_idx,
                'image_id': image_id,
                'image_relation_idx': image_relation_idx,
                'subject': subject_obj['class_name'],
                'object': object_obj['class_name'],
                'gt_predicate': gt_predicate
            })
            
            # å°†è¯¥å…³ç³»çš„50ä¸ªè°“è¯å€™é€‰åŠ å…¥è¯¥å›¾ç‰‡çš„å€™é€‰æ± 
            for pred_score in predicate_scores:
                image_candidates.append({
                    'relation_idx': image_relation_idx,  # ä½¿ç”¨å›¾ç‰‡å†…çš„å…³ç³»ç´¢å¼•
                    'global_relation_idx': global_relation_idx,
                    'image_id': image_id,
                    'subject': subject_obj['class_name'],
                    'object': object_obj['class_name'],
                    'gt_predicate': gt_predicate,
                    'predicted_predicate': pred_score['predicate'],
                    'similarity': pred_score['similarity'],
                    'is_correct': (pred_score['predicate'] == gt_predicate)
                })
            
            image_relation_idx += 1
            global_relation_idx += 1
        
        # ä¿å­˜è¯¥å›¾ç‰‡çš„æ‰€æœ‰å€™é€‰
        per_image_candidates[image_id] = image_candidates
    
    print(f"\nâœ… é¢„æµ‹å®Œæˆï¼")
    print(f"   æ€»å›¾ç‰‡æ•°: {len(per_image_candidates)}")
    print(f"   æ€»å…³ç³»æ•°: {len(all_relations_info)}")
    total_candidates = sum(len(candidates) for candidates in per_image_candidates.values())
    print(f"   æ€»å€™é€‰é¢„æµ‹æ•°: {total_candidates}")
    
    # 4. è®¡ç®—Per-Image Recall@50å¹¶å–å¹³å‡
    print("\nğŸ“Š è®¡ç®—Per-Image Recall@50ï¼ˆæ¯å¼ å›¾ç‰‡ç‹¬ç«‹è®¡ç®—å†å¹³å‡ï¼‰...")
    recall_results = calculate_average_recall_at_k(per_image_candidates, k=50)
    
    print("\n" + "="*80)
    print("è¯„ä¼°ç»“æœ (Per-Image Recall@50)")
    print("="*80)
    print(f"å¹³å‡ Recall@{recall_results['k']}: {recall_results['avg_recall@k']:.4f} ({recall_results['avg_recall@k']*100:.2f}%)")
    print(f"æ€»å›¾ç‰‡æ•°: {recall_results['total_images']}")
    print(f"æ€»å¬å›å…³ç³»æ•°: {recall_results['total_recalled_relations']}/{recall_results['total_gt_relations']}")
    print("="*80)
    
    # 5. æ˜¾ç¤ºæ¯å¼ å›¾ç‰‡çš„recallåˆ†å¸ƒ
    per_image_recalls = [r['recall@k'] for r in recall_results['per_image_results']]
    if per_image_recalls:
        print(f"\nRecallåˆ†å¸ƒç»Ÿè®¡:")
        print(f"  æœ€å¤§å€¼: {max(per_image_recalls):.4f}")
        print(f"  æœ€å°å€¼: {min(per_image_recalls):.4f}")
        print(f"  ä¸­ä½æ•°: {sorted(per_image_recalls)[len(per_image_recalls)//2]:.4f}")
    
    # 6. æ”¶é›†æ‰€æœ‰å€™é€‰ç”¨äºå±•ç¤ºï¼ˆå¯é€‰ï¼‰
    all_candidate_predictions = []
    for candidates in per_image_candidates.values():
        all_candidate_predictions.extend(candidates)
    
    candidates_sorted = sorted(all_candidate_predictions, key=lambda x: x['similarity'], reverse=True)
    top50_global_candidates = candidates_sorted[:50]
    
    # 7. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜ç»“æœåˆ°: {OUTPUT_FILE}")
    output_data = {
        'summary': {
            'evaluation_method': 'per-image',
            'total_images': len(per_image_candidates),
            'total_relations': len(all_relations_info),
            'total_candidates': total_candidates,
            'avg_recall@50': recall_results['avg_recall@k'],
            'total_recalled_relations': recall_results['total_recalled_relations'],
            'total_gt_relations': recall_results['total_gt_relations']
        },
        'per_image_results': recall_results['per_image_results'],
        'all_relations': all_relations_info,
        'top50_global_candidates': top50_global_candidates,  # å…¨å±€æ’åºçš„top50ï¼ˆå‚è€ƒç”¨ï¼‰
        # 'all_candidates': all_candidate_predictions  # å®Œæ•´çš„å€™é€‰åˆ—è¡¨ï¼ˆå¯é€‰ï¼Œå¯èƒ½å¾ˆå¤§ï¼‰
    }
    
    os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    
    print("âœ… ç»“æœå·²ä¿å­˜ï¼")
    
    # 8. æ˜¾ç¤ºä¸€äº›æ ·ä¾‹
    print("\n" + "="*80)
    print("Per-Image Recallæ ·ä¾‹ï¼ˆå‰5å¼ å›¾ç‰‡ï¼‰")
    print("="*80)
    for i, img_result in enumerate(recall_results['per_image_results'][:5], 1):
        print(f"\n{i}. å›¾ç‰‡#{img_result['image_id']}")
        print(f"   Recall@50: {img_result['recall@k']:.4f} ({img_result['recall@k']*100:.2f}%)")
        print(f"   å¬å›: {img_result['recalled_relations']}/{img_result['total_gt_relations']} å…³ç³»")
        print(f"   å€™é€‰æ•°: {img_result['total_candidates']} (Top-{img_result['k']}ä¸­å–{img_result['top_k_candidates']}ä¸ª)")
    
    print("\n" + "="*80)
    print("å…¨å±€Top-50å€™é€‰é¢„æµ‹æ ·ä¾‹ï¼ˆå‰10ä¸ªï¼Œä»…ä¾›å‚è€ƒï¼‰")
    print("="*80)
    for i, pred in enumerate(top50_global_candidates[:10], 1):
        status = "âœ…" if pred['is_correct'] else "âŒ"
        print(f"\n{i}. {status} æ’å#{i} (ç›¸ä¼¼åº¦: {pred['similarity']:.4f})")
        print(f"   å›¾ç‰‡#{pred['image_id']}, å…³ç³»#{pred['image_relation_idx']}: {pred['subject']} --[{pred['predicted_predicate']}]--> {pred['object']}")
        print(f"   GTè°“è¯: {pred['gt_predicate']}")


if __name__ == "__main__":
    main()


    