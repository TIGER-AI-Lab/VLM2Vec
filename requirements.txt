accelerate
datasets
decord
einops
hjson
hnswlib
huggingface-hub
ninja
numpy==1.26.4
opencv-contrib-python
opencv-python
peft
pillow
py-cpuinfo
pytrec-eval
qwen-vl-utils[decord]==0.0.8  # https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/discussions/30
ray
scikit-image
scikit-learn
scipy
sentencepiece
timm
torch
torchvision
tqdm
transformers==4.52.3
wandb
wrapt

# pip install flash-attn --no-build-isolation
# Installed separately. check out orginal repo README for accelerating installation: https://github.com/Dao-AILab/flash-attention
