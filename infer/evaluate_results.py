"""
è¯„ä¼°åœºæ™¯å›¾å…³ç³»é¢„æµ‹ç»“æœçš„è„šæœ¬
ç”¨äºåˆ†æ predict_scene_graph_recall.py è¾“å‡ºçš„ JSON æ–‡ä»¶
"""

import json
import argparse
from collections import defaultdict
import numpy as np
from typing import Dict, List, Tuple


def load_results(json_path: str) -> Dict:
    """åŠ è½½é¢„æµ‹ç»“æœJSONæ–‡ä»¶"""
    print(f"ğŸ“– æ­£åœ¨åŠ è½½ç»“æœæ–‡ä»¶: {json_path}")
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    print(f"âœ… åŠ è½½å®Œæˆ\n")
    return data


def calculate_mean_rank(data: Dict) -> Dict:
    """
    è®¡ç®—Mean Rank (MR) æŒ‡æ ‡
    
    å¯¹äºæ¯ä¸ªGTå…³ç³»ï¼Œæ‰¾åˆ°æ­£ç¡®è°“è¯åœ¨æ‰€æœ‰å€™é€‰ä¸­çš„æ’åï¼Œç„¶åæ±‚å¹³å‡
    """
    print("ğŸ“Š è®¡ç®— Mean Rank (MR) æŒ‡æ ‡...")
    
    # ä¼˜å…ˆä½¿ç”¨ per_image_top100_candidatesï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ all_candidates
    all_candidates = []
    
    if 'per_image_top100_candidates' in data:
        print("   ä½¿ç”¨ per_image_top100_candidates å­—æ®µ...")
        for image_id, candidates in data['per_image_top100_candidates'].items():
            all_candidates.extend(candidates)
    elif 'all_candidates' in data:
        print("   ä½¿ç”¨ all_candidates å­—æ®µ...")
        all_candidates = data['all_candidates']
    else:
        print("âš ï¸  JSONä¸­æ²¡æœ‰ä¿å­˜å€™é€‰åˆ—è¡¨ (per_image_top100_candidates æˆ– all_candidates å­—æ®µç¼ºå¤±)")
        print("   æ— æ³•è®¡ç®— Mean Rank\n")
        return None
    
    # æŒ‰ (image_id, relation_idx) åˆ†ç»„ï¼ˆåªç»Ÿè®¡æœ‰æ•ˆçš„GTå…³ç³»ï¼Œæ’é™¤-1ï¼‰
    relation_candidates = defaultdict(list)
    for cand in all_candidates:
        if cand['relation_idx'] >= 0:  # åªç»Ÿè®¡æœ‰æ•ˆçš„GTå…³ç³»
            key = (cand['image_id'], cand['relation_idx'])
            relation_candidates[key].append(cand)
    
    ranks = []
    rank_distribution = defaultdict(int)
    
    for key, candidates in relation_candidates.items():
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        sorted_candidates = sorted(candidates, key=lambda x: x['similarity'], reverse=True)
        
        # æ‰¾åˆ°æ­£ç¡®é¢„æµ‹çš„æ’åï¼ˆç¬¬ä¸€ä¸ªæ­£ç¡®çš„ä½ç½®ï¼‰ï¼Œæ’é™¤no relationé¢„æµ‹
        correct_rank = None
        for rank, cand in enumerate(sorted_candidates, 1):
            if cand['is_correct'] and cand.get('predicted_predicate') != 'no relation':
                correct_rank = rank
                break
        
        if correct_rank is not None:
            ranks.append(correct_rank)
            rank_distribution[correct_rank] += 1
    
    mean_rank = np.mean(ranks) if ranks else 0.0
    median_rank = np.median(ranks) if ranks else 0.0
    
    # è®¡ç®— MRR (Mean Reciprocal Rank)
    reciprocal_ranks = [1.0 / r for r in ranks]
    mrr = np.mean(reciprocal_ranks) if reciprocal_ranks else 0.0
    
    print(f"âœ… Mean Rank (MR): {mean_rank:.2f}")
    print(f"âœ… Median Rank: {median_rank:.2f}")
    print(f"âœ… Mean Reciprocal Rank (MRR): {mrr:.4f}")
    print(f"   ç»Ÿè®¡äº† {len(ranks)} ä¸ªå…³ç³»çš„æ’å\n")
    
    return {
        'mean_rank': mean_rank,
        'median_rank': median_rank,
        'mrr': mrr,
        'total_relations': len(ranks),
        'rank_distribution': dict(sorted(rank_distribution.items()))
    }


def calculate_recall_at_multiple_k(data: Dict, k_values: List[int] = [1, 5, 10, 20, 50, 100]) -> Dict:
    """
    è®¡ç®—å¤šä¸ªKå€¼ä¸‹çš„Recall@K
    
    æ³¨æ„ï¼šè¿™éœ€è¦å€™é€‰åˆ—è¡¨ (per_image_top100_candidates æˆ– all_candidates)
    ä¿®æ”¹ï¼šå…ˆè¿‡æ»¤no relationï¼Œå†å–top-kï¼Œç„¶ååªä¿ç•™GTä¸­å­˜åœ¨çš„å…³ç³»å¯¹è¿›è¡Œè¯„ä¼°ï¼ˆPredClsè®¾ç½®ï¼‰
    """
    print(f"ğŸ“Š è®¡ç®—å¤šä¸ªKå€¼ä¸‹çš„ Recall@K (å…ˆè¿‡æ»¤no relationï¼Œå†å–top-k): {k_values}")
    
    # ä¼˜å…ˆä½¿ç”¨ per_image_top100_candidates
    per_image_candidates = {}
    
    if 'per_image_top100_candidates' in data:
        print("   ä½¿ç”¨ per_image_top100_candidates å­—æ®µ...")
        per_image_candidates = data['per_image_top100_candidates']
        # æ£€æŸ¥Kå€¼æ˜¯å¦è¶…è¿‡100
        max_k = max(k_values)
        if max_k > 100:
            print(f"âš ï¸  è­¦å‘Š: Kå€¼ {max_k} è¶…è¿‡äº†ä¿å­˜çš„Top-100å€™é€‰ï¼Œç»“æœå¯èƒ½ä¸å‡†ç¡®")
    elif 'all_candidates' in data:
        print("   ä½¿ç”¨ all_candidates å­—æ®µ...")
        all_candidates = data['all_candidates']
        # æŒ‰ image_id åˆ†ç»„
        per_image_candidates_list = defaultdict(list)
        for cand in all_candidates:
            per_image_candidates_list[cand['image_id']].append(cand)
        per_image_candidates = dict(per_image_candidates_list)
    else:
        print("âš ï¸  JSONä¸­æ²¡æœ‰ä¿å­˜å€™é€‰åˆ—è¡¨ï¼Œæ— æ³•è®¡ç®—ä¸åŒKå€¼çš„Recall\n")
        return None
    
    results = {}
    
    for k in k_values:
        total_recall = 0.0
        valid_images = 0
        total_gt_relations = 0
        total_recalled_relations = 0
        
        for image_id, candidates in per_image_candidates.items():
            # è·å–è¯¥å›¾ç‰‡ä¸­æ‰€æœ‰GTå…³ç³»ï¼ˆåªç»Ÿè®¡relation_idx >= 0çš„ï¼Œæ’é™¤-1ï¼‰
            gt_relations = set()
            for cand in candidates:
                if cand['relation_idx'] >= 0:  # åªç»Ÿè®¡æœ‰æ•ˆçš„GTå…³ç³»
                    gt_relations.add(cand['relation_idx'])
            
            # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰no relationçš„é¢„æµ‹ï¼ˆä»æ‰€æœ‰å€™é€‰ä¸­ï¼‰
            non_bg_candidates = []
            for cand in candidates:
                if cand.get('predicted_predicate') != 'no relation':
                    non_bg_candidates.append(cand)
            
            # ç¬¬äºŒæ­¥ï¼šæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
            sorted_candidates = sorted(non_bg_candidates, key=lambda x: x['similarity'], reverse=True)
            top_k = sorted_candidates[:min(k, len(sorted_candidates))]
            
            # ç¬¬ä¸‰æ­¥ï¼šåœ¨top-kä¸­ï¼Œåªå¯¹GTå…³ç³»å¯¹è¿›è¡Œè¯„ä¼°ï¼Œç»Ÿè®¡å¬å›çš„å…³ç³»ï¼ˆå»é‡ï¼‰
            recalled_relations = set()
            for cand in top_k:
                # åªç»Ÿè®¡GTå…³ç³»å¯¹ä¸­é¢„æµ‹æ­£ç¡®çš„
                if cand['relation_idx'] in gt_relations and cand['is_correct']:
                    recalled_relations.add(cand['relation_idx'])
            
            # è®¡ç®—è¯¥å›¾ç‰‡çš„recall
            total_gt_in_image = len(gt_relations)
            recalled_in_image = len(recalled_relations)
            
            recall = recalled_in_image / total_gt_in_image if total_gt_in_image > 0 else 0.0
            total_recall += recall
            valid_images += 1
            total_gt_relations += total_gt_in_image
            total_recalled_relations += recalled_in_image
        
        avg_recall = total_recall / valid_images if valid_images > 0 else 0.0
        overall_recall = total_recalled_relations / total_gt_relations if total_gt_relations > 0 else 0.0
        
        results[f'recall@{k}'] = avg_recall
        results[f'overall_recall@{k}'] = overall_recall
        results[f'stats@{k}'] = {
            'total_gt_relations': total_gt_relations,
            'total_recalled_relations': total_recalled_relations,
            'valid_images': valid_images
        }
        
        print(f"   Recall@{k:3d}: {avg_recall:.4f} ({avg_recall*100:.2f}%) [å¹³å‡]")
        print(f"   Overall@{k:3d}: {overall_recall:.4f} ({overall_recall*100:.2f}%) [æ•´ä½“]")
        print(f"   ç»Ÿè®¡@{k:3d}: {total_recalled_relations}/{total_gt_relations} å…³ç³»è¢«å¬å›ï¼Œ{valid_images} å¼ å›¾ç‰‡")
    
    print()
    return results


def calculate_category_recall_at_k(data: Dict, k_values: List[int] = [1, 5, 10, 20, 50, 100]) -> Dict:
    """
    è®¡ç®—baseå’Œnovelè°“è¯ç±»åˆ«çš„Recall@K
    
    Args:
        data: é¢„æµ‹ç»“æœæ•°æ®
        k_values: Kå€¼åˆ—è¡¨
    
    Returns:
        å­—å…¸ï¼ŒåŒ…å«baseå’Œnovelç±»åˆ«çš„recallç»Ÿè®¡
    ä¿®æ”¹ï¼šå…ˆè¿‡æ»¤no relationï¼Œå†å–top-kï¼Œç„¶ååªä¿ç•™GTä¸­å­˜åœ¨çš„å…³ç³»å¯¹è¿›è¡Œè¯„ä¼°ï¼ˆPredClsè®¾ç½®ï¼‰
    """
    print(f"ğŸ“Š è®¡ç®—Baseå’ŒNovelè°“è¯ç±»åˆ«çš„Recall@K (å…ˆè¿‡æ»¤no relationï¼Œå†å–top-k): {k_values}")
    
    # åŠ è½½è°“è¯ç±»åˆ«æ˜ å°„
    predicate_category_mapping = {
        "above": "base", "across": "novel", "against": "base", "along": "novel", "and": "novel",
        "at": "base", "attached to": "base", "behind": "base", "belonging to": "base", "between": "base",
        "carrying": "base", "covered in": "base", "covering": "base", "eating": "novel", "flying in": "novel",
        "for": "base", "from": "base", "growing on": "novel", "hanging from": "base", "has": "base",
        "holding": "base", "in": "base", "in front of": "base", "laying on": "novel", "looking at": "base",
        "lying on": "novel", "made of": "base", "mounted on": "novel", "near": "base", "of": "base",
        "on": "base", "on back of": "novel", "over": "base", "painted on": "novel", "parked on": "base",
        "part of": "novel", "playing": "base", "riding": "base", "says": "novel", "sitting on": "base",
        "standing on": "base", "to": "base", "under": "base", "using": "novel", "walking in": "novel",
        "walking on": "base", "watching": "base", "wearing": "base", "wears": "base", "with": "base"
    }
    
    # ä¼˜å…ˆä½¿ç”¨ per_image_top100_candidates
    per_image_candidates = {}
    
    if 'per_image_top100_candidates' in data:
        print("   ä½¿ç”¨ per_image_top100_candidates å­—æ®µ...")
        per_image_candidates = data['per_image_top100_candidates']
        max_k = max(k_values)
        if max_k > 100:
            print(f"âš ï¸  è­¦å‘Š: Kå€¼ {max_k} è¶…è¿‡äº†ä¿å­˜çš„Top-100å€™é€‰ï¼Œç»“æœå¯èƒ½ä¸å‡†ç¡®")
    elif 'all_candidates' in data:
        print("   ä½¿ç”¨ all_candidates å­—æ®µ...")
        all_candidates = data['all_candidates']
        per_image_candidates_list = defaultdict(list)
        for cand in all_candidates:
            per_image_candidates_list[cand['image_id']].append(cand)
        per_image_candidates = dict(per_image_candidates_list)
    else:
        print("âš ï¸  JSONä¸­æ²¡æœ‰ä¿å­˜å€™é€‰åˆ—è¡¨ï¼Œæ— æ³•è®¡ç®—ç±»åˆ«recall\n")
        return None
    
    results = {}
    
    for k in k_values:
        # åˆå§‹åŒ–æ¯ä¸ªè°“è¯çš„ç»Ÿè®¡ï¼ˆæŒ‰è°“è¯åˆ†ç±»ï¼‰
        predicate_stats = {}
        for pred_name, category in predicate_category_mapping.items():
            predicate_stats[pred_name] = {'hit': 0, 'total': 0, 'category': category}
        
        for image_id, candidates in per_image_candidates.items():
            # è·å–è¯¥å›¾ç‰‡ä¸­æ‰€æœ‰GTå…³ç³»ï¼ˆåªç»Ÿè®¡relation_idx >= 0çš„ï¼Œæ’é™¤-1ï¼‰
            gt_relations = set()
            for cand in candidates:
                if cand['relation_idx'] >= 0:  # åªç»Ÿè®¡æœ‰æ•ˆçš„GTå…³ç³»
                    gt_relations.add(cand['relation_idx'])
            
            # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰no relationçš„é¢„æµ‹ï¼ˆä»æ‰€æœ‰å€™é€‰ä¸­ï¼‰
            non_bg_candidates = []
            for cand in candidates:
                if cand.get('predicted_predicate') != 'no relation':
                    non_bg_candidates.append(cand)
            
            # ç¬¬äºŒæ­¥ï¼šæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
            predictions_sorted = sorted(non_bg_candidates, key=lambda x: x['similarity'], reverse=True)
            actual_k = min(k, len(predictions_sorted))
            top_k_predictions = predictions_sorted[:actual_k]
            
            # ç»Ÿè®¡è¯¥å›¾ç‰‡ä¸­æ¯ä¸ªè°“è¯çš„GT
            gt_predicates_in_image = {}
            recalled_predicates_in_image = {}
            
            for cand in candidates:
                gt_pred = cand.get('gt_predicate')
                if gt_pred is None or gt_pred not in predicate_stats:
                    continue
                relation_idx = cand['relation_idx']
                
                # ç»Ÿè®¡GTï¼ˆæ¯ä¸ªå…³ç³»åªç®—ä¸€æ¬¡ï¼Œåªç»Ÿè®¡æœ‰æ•ˆçš„GTå…³ç³»ï¼Œæ’é™¤-1ï¼‰
                if relation_idx >= 0 and relation_idx not in gt_predicates_in_image:
                    gt_predicates_in_image[relation_idx] = gt_pred
                    predicate_stats[gt_pred]['total'] += 1
            
            # ç¬¬ä¸‰æ­¥ï¼šåœ¨top-kä¸­ï¼Œåªå¯¹GTå…³ç³»å¯¹è¿›è¡Œè¯„ä¼°ï¼Œç»Ÿè®¡å¬å›çš„è°“è¯
            for cand in top_k_predictions:
                # åªç»Ÿè®¡GTå…³ç³»å¯¹ä¸­é¢„æµ‹æ­£ç¡®çš„
                if cand['relation_idx'] in gt_relations and cand['is_correct']:
                    relation_idx = cand['relation_idx']
                    gt_pred = cand.get('gt_predicate')
                    if gt_pred is None or gt_pred not in predicate_stats:
                        continue
                    
                    # æ¯ä¸ªå…³ç³»åªç®—ä¸€æ¬¡å¬å›
                    if relation_idx not in recalled_predicates_in_image:
                        recalled_predicates_in_image[relation_idx] = gt_pred
                        predicate_stats[gt_pred]['hit'] += 1
        
        # è®¡ç®—æ¯ä¸ªè°“è¯çš„recallï¼Œç„¶åæŒ‰ç±»åˆ«åˆ†ç»„å¹³å‡
        base_recalls = []
        novel_recalls = []
        base_total = 0
        novel_total = 0
        base_hit = 0
        novel_hit = 0
        
        for pred, stats in predicate_stats.items():
            if stats['total'] > 0:
                pred_recall = stats['hit'] / stats['total']
                if stats['category'] == 'base':
                    base_recalls.append(pred_recall)
                    base_total += stats['total']
                    base_hit += stats['hit']
                elif stats['category'] == 'novel':
                    novel_recalls.append(pred_recall)
                    novel_total += stats['total']
                    novel_hit += stats['hit']
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„mean recallï¼ˆç®€å•å¹³å‡ï¼Œä¸åŠ æƒï¼‰
        category_recall = {}
        category_recall['base'] = {
            'recall': np.mean(base_recalls) if base_recalls else 0.0,
            'hit': base_hit,
            'total': base_total,
            'num_predicates': len(base_recalls)
        }
        category_recall['novel'] = {
            'recall': np.mean(novel_recalls) if novel_recalls else 0.0,
            'hit': novel_hit,
            'total': novel_total,
            'num_predicates': len(novel_recalls)
        }
        
        results[f'category_recall@{k}'] = category_recall
        
        # æ‰“å°ç»“æœ
        base_recall = category_recall['base']['recall']
        novel_recall = category_recall['novel']['recall']
        base_info = f"{category_recall['base']['hit']}/{category_recall['base']['total']}, {category_recall['base']['num_predicates']}è°“è¯"
        novel_info = f"{category_recall['novel']['hit']}/{category_recall['novel']['total']}, {category_recall['novel']['num_predicates']}è°“è¯"
        
        print(f"   Recall@{k:3d} - Base: {base_recall:.4f} ({base_recall*100:.2f}%) [{base_info}]")
        print(f"   Recall@{k:3d} - Novel: {novel_recall:.4f} ({novel_recall*100:.2f}%) [{novel_info}]")
    
    print()
    return results


def calculate_mean_recall_per_predicate_multi_k(data: Dict, k_values: List[int] = [1, 5, 10, 20, 50, 100]) -> Dict:
    """
    è®¡ç®—å¤šä¸ªKå€¼ä¸‹æ¯ä¸ªè°“è¯ç±»åˆ«çš„Mean Recall
    
    Args:
        data: é¢„æµ‹ç»“æœæ•°æ®
        k_values: Kå€¼åˆ—è¡¨
    
    Returns:
        å­—å…¸ï¼ŒåŒ…å«æ¯ä¸ªKå€¼ä¸‹çš„è°“è¯MRç»Ÿè®¡
    ä¿®æ”¹ï¼šå…ˆè¿‡æ»¤no relationï¼Œå†å–top-kï¼Œç„¶ååªä¿ç•™GTä¸­å­˜åœ¨çš„å…³ç³»å¯¹è¿›è¡Œè¯„ä¼°ï¼ˆPredClsè®¾ç½®ï¼‰
    """
    print(f"ğŸ“Š è®¡ç®—å¤šä¸ªKå€¼ä¸‹çš„è°“è¯çº§åˆ« Mean Recall (å…ˆè¿‡æ»¤no relationï¼Œå†å–top-k): {k_values}")
    
    # ä¼˜å…ˆä½¿ç”¨ per_image_top100_candidates
    per_image_candidates = {}
    
    if 'per_image_top100_candidates' in data:
        print("   ä½¿ç”¨ per_image_top100_candidates å­—æ®µ...")
        per_image_candidates = data['per_image_top100_candidates']
        max_k = max(k_values)
        if max_k > 100:
            print(f"âš ï¸  è­¦å‘Š: Kå€¼ {max_k} è¶…è¿‡äº†ä¿å­˜çš„Top-100å€™é€‰ï¼Œç»“æœå¯èƒ½ä¸å‡†ç¡®")
    elif 'all_candidates' in data:
        print("   ä½¿ç”¨ all_candidates å­—æ®µ...")
        all_candidates = data['all_candidates']
        per_image_candidates_list = defaultdict(list)
        for cand in all_candidates:
            per_image_candidates_list[cand['image_id']].append(cand)
        per_image_candidates = dict(per_image_candidates_list)
    else:
        print("âš ï¸  JSONä¸­æ²¡æœ‰ä¿å­˜å€™é€‰åˆ—è¡¨ï¼Œæ— æ³•è®¡ç®—è°“è¯MR\n")
        return None
    
    # è·å–æ‰€æœ‰è°“è¯åˆ—è¡¨ï¼ˆè¿‡æ»¤Noneå€¼ï¼‰
    predicates_set = set()
    for candidates in per_image_candidates.values():
        for cand in candidates:
            predicate = cand.get('gt_predicate')
            if predicate is not None:  # è¿‡æ»¤Noneå€¼
                predicates_set.add(predicate)
    predicates = sorted(list(predicates_set))
    
    results = {}
    
    for k in k_values:
        # åˆå§‹åŒ–æ¯ä¸ªè°“è¯çš„ç»Ÿè®¡
        predicate_stats = {pred: {'hit': 0, 'total': 0} for pred in predicates}
        
        for image_id, candidates in per_image_candidates.items():
            # è·å–è¯¥å›¾ç‰‡ä¸­æ‰€æœ‰GTå…³ç³»ï¼ˆåªç»Ÿè®¡relation_idx >= 0çš„ï¼Œæ’é™¤-1ï¼‰
            gt_relations = set()
            for cand in candidates:
                if cand['relation_idx'] >= 0:  # åªç»Ÿè®¡æœ‰æ•ˆçš„GTå…³ç³»
                    gt_relations.add(cand['relation_idx'])
            
            # ç¬¬ä¸€æ­¥ï¼šè¿‡æ»¤æ‰no relationçš„é¢„æµ‹ï¼ˆä»æ‰€æœ‰å€™é€‰ä¸­ï¼‰
            non_bg_candidates = []
            for cand in candidates:
                if cand.get('predicted_predicate') != 'no relation':
                    non_bg_candidates.append(cand)
            
            # ç¬¬äºŒæ­¥ï¼šæŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–top-k
            predictions_sorted = sorted(non_bg_candidates, key=lambda x: x['similarity'], reverse=True)
            actual_k = min(k, len(predictions_sorted))
            top_k_predictions = predictions_sorted[:actual_k]
            
            # ç»Ÿè®¡è¯¥å›¾ç‰‡ä¸­æ¯ä¸ªè°“è¯ç±»åˆ«çš„GT
            gt_predicates_in_image = {}
            recalled_predicates_in_image = {}
            
            for cand in candidates:
                gt_pred = cand.get('gt_predicate')
                if gt_pred is None:  # è·³è¿‡Noneå€¼
                    continue
                relation_idx = cand['relation_idx']
                
                # ç»Ÿè®¡GTï¼ˆæ¯ä¸ªå…³ç³»åªç®—ä¸€æ¬¡ï¼Œåªç»Ÿè®¡æœ‰æ•ˆçš„GTå…³ç³»ï¼Œæ’é™¤-1ï¼‰
                if relation_idx >= 0 and relation_idx not in gt_predicates_in_image:
                    gt_predicates_in_image[relation_idx] = gt_pred
                    predicate_stats[gt_pred]['total'] += 1
            
            # ç¬¬ä¸‰æ­¥ï¼šåœ¨top-kä¸­ï¼Œåªå¯¹GTå…³ç³»å¯¹è¿›è¡Œè¯„ä¼°ï¼Œç»Ÿè®¡å¬å›çš„è°“è¯
            for cand in top_k_predictions:
                # åªç»Ÿè®¡GTå…³ç³»å¯¹ä¸­é¢„æµ‹æ­£ç¡®çš„
                if cand['relation_idx'] in gt_relations and cand['is_correct']:
                    relation_idx = cand['relation_idx']
                    gt_pred = cand.get('gt_predicate')
                    if gt_pred is None:  # è·³è¿‡Noneå€¼
                        continue
                    
                    # æ¯ä¸ªå…³ç³»åªç®—ä¸€æ¬¡å¬å›
                    if relation_idx not in recalled_predicates_in_image:
                        recalled_predicates_in_image[relation_idx] = gt_pred
                        predicate_stats[gt_pred]['hit'] += 1
        
        # è®¡ç®—æ¯ä¸ªè°“è¯çš„recall
        per_predicate_recall = {}
        valid_recalls = []
        
        for pred in predicates:
            total = predicate_stats[pred]['total']
            hit = predicate_stats[pred]['hit']
            
            if total > 0:
                recall = hit / total
                per_predicate_recall[pred] = {
                    'recall': recall,
                    'hit': hit,
                    'total': total
                }
                valid_recalls.append(recall)
            else:
                per_predicate_recall[pred] = {
                    'recall': 0.0,
                    'hit': 0,
                    'total': 0
                }
        
        # è®¡ç®—mean recallï¼ˆåªå¯¹æœ‰GTçš„ç±»åˆ«è®¡ç®—ï¼‰
        mean_recall = np.mean(valid_recalls) if valid_recalls else 0.0
        
        results[f'mean_recall@{k}'] = {
            'mean_recall': mean_recall,
            'num_valid_predicates': len(valid_recalls),
            'total_predicates': len(predicates),
            'per_predicate_recall': per_predicate_recall
        }
        
        print(f"   Mean Recall@{k:3d}: {mean_recall:.4f} ({mean_recall*100:.2f}%), æœ‰æ•ˆè°“è¯: {len(valid_recalls)}/{len(predicates)}")
    
    print()
    return results


def display_category_recall_results(category_recall_results: Dict) -> None:
    """
    å±•ç¤ºBaseå’ŒNovelè°“è¯ç±»åˆ«çš„Recallç»“æœ
    
    Args:
        category_recall_results: ç±»åˆ«recallç»“æœ
    """
    if not category_recall_results:
        return
    
    print("="*80)
    print("ğŸ“Š Baseå’ŒNovelè°“è¯ç±»åˆ«Recallè¯¦ç»†åˆ†æ")
    print("="*80)
    
    # æå–æ‰€æœ‰kå€¼å¹¶æ’åº
    k_values = sorted([int(k.split('@')[1]) for k in category_recall_results.keys()])
    
    # æ‰“å°æ€»ä½“è¶‹åŠ¿
    print("\nğŸ“ˆ Base vs Novel Recallå¯¹æ¯”:")
    print(f"{'Kå€¼':<10}{'Base Recall':<15}{'Novel Recall':<15}{'Baseè°“è¯æ•°':<15}{'Novelè°“è¯æ•°':<15}")
    print("-"*70)
    
    for k in k_values:
        key = f'category_recall@{k}'
        base_recall = category_recall_results[key]['base']['recall']
        novel_recall = category_recall_results[key]['novel']['recall']
        base_num = category_recall_results[key]['base']['num_predicates']
        novel_num = category_recall_results[key]['novel']['num_predicates']
        
        print(f"K={k:<8}{base_recall:<15.4f}{novel_recall:<15.4f}{base_num:<15}{novel_num:<15}")
    
    # åˆ†ææ€§èƒ½å·®å¼‚
    print(f"\nğŸ“Š æ€§èƒ½å·®å¼‚åˆ†æ:")
    for k in k_values:
        key = f'category_recall@{k}'
        base_recall = category_recall_results[key]['base']['recall']
        novel_recall = category_recall_results[key]['novel']['recall']
        
        if base_recall > 0 and novel_recall > 0:
            ratio = base_recall / novel_recall
            diff = base_recall - novel_recall
            
            print(f"K={k}: Base/Novelæ¯”ç‡ = {ratio:.2f}, å·®å¼‚ = {diff:+.4f}")
            
            if ratio > 1.2:
                print(f"   â†’ Baseè°“è¯è¡¨ç°æ˜æ˜¾æ›´å¥½")
            elif ratio < 0.8:
                print(f"   â†’ Novelè°“è¯è¡¨ç°æ˜æ˜¾æ›´å¥½")
            else:
                print(f"   â†’ ä¸¤ç±»è°“è¯è¡¨ç°ç›¸å½“")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    base_nums = [category_recall_results[f'category_recall@{k}']['base']['num_predicates'] for k in k_values]
    novel_nums = [category_recall_results[f'category_recall@{k}']['novel']['num_predicates'] for k in k_values]
    base_totals = [category_recall_results[f'category_recall@{k}']['base']['total'] for k in k_values]
    novel_totals = [category_recall_results[f'category_recall@{k}']['novel']['total'] for k in k_values]
    
    print(f"Baseè°“è¯ç±»åˆ«æ•°: {max(base_nums) if base_nums else 0}")
    print(f"Novelè°“è¯ç±»åˆ«æ•°: {max(novel_nums) if novel_nums else 0}")
    print(f"Baseå…³ç³»å®ä¾‹æ•°: {max(base_totals) if base_totals else 0}")
    print(f"Novelå…³ç³»å®ä¾‹æ•°: {max(novel_totals) if novel_totals else 0}")
    
    if base_nums and novel_nums:
        base_novel_ratio = max(base_nums) / max(novel_nums) if max(novel_nums) > 0 else 0
        print(f"Base/Novelè°“è¯æ•°é‡æ¯”: {base_novel_ratio:.2f}")
    
    print()


def display_predicate_multi_k_results(predicate_multi_k_results: Dict, top_n: int = 10, detail_k_values: List[int] = None) -> None:
    """
    å±•ç¤ºå¤šKå€¼ä¸‹çš„è°“è¯çº§åˆ«Mean Recallç»“æœ
    
    Args:
        predicate_multi_k_results: å¤šKå€¼ä¸‹çš„è°“è¯MRç»“æœ
        top_n: æ˜¾ç¤ºTop-Nå’ŒBottom-Nçš„è°“è¯
        detail_k_values: éœ€è¦æ˜¾ç¤ºè¯¦ç»†è°“è¯ä¿¡æ¯çš„Kå€¼åˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºåªæ˜¾ç¤ºæ€»ä½“è¶‹åŠ¿
    """
    if not predicate_multi_k_results:
        return
    
    print("="*80)
    print("ğŸ“Š å¤šKå€¼ä¸‹çš„è°“è¯çº§åˆ« Mean Recall è¯¦ç»†åˆ†æ")
    print("="*80)
    
    # æå–æ‰€æœ‰kå€¼å¹¶æ’åº
    k_values = sorted([int(k.split('@')[1]) for k in predicate_multi_k_results.keys()])
    
    # æ‰“å°æ€»ä½“Mean Recallè¶‹åŠ¿
    print("\nğŸ“ˆ æ€»ä½“Mean Recallè¶‹åŠ¿:")
    print(f"{'Kå€¼':<10}{'Mean Recall':<15}{'ç™¾åˆ†æ¯”':<12}{'æœ‰æ•ˆè°“è¯æ•°':<15}")
    print("-"*52)
    for k in k_values:
        key = f'mean_recall@{k}'
        mr = predicate_multi_k_results[key]['mean_recall']
        valid = predicate_multi_k_results[key]['num_valid_predicates']
        total = predicate_multi_k_results[key]['total_predicates']
        print(f"K={k:<8}{mr:<15.4f}{mr*100:<12.2f}{valid}/{total}")
    
        # å¦‚æœæœªæŒ‡å®šdetail_k_valuesï¼Œåˆ™é»˜è®¤æ˜¾ç¤ºæ‰€æœ‰Kå€¼çš„è¯¦ç»†ä¿¡æ¯
        if detail_k_values is None:
            # æ˜¾ç¤ºæ‰€æœ‰Kå€¼
            detail_k_values = k_values
    
    # åªä¸ºæŒ‡å®šçš„Kå€¼æ˜¾ç¤ºè¯¦ç»†çš„Top-Nå’ŒBottom-Nè°“è¯
    for k in detail_k_values:
        if k not in k_values:
            continue
            
        key = f'mean_recall@{k}'
        per_predicate = predicate_multi_k_results[key]['per_predicate_recall']
        
        # è¿‡æ»¤æœ‰GTçš„è°“è¯å¹¶æ’åº
        predicates_with_gt = [(pred, stats) for pred, stats in per_predicate.items() if stats['total'] > 0]
        sorted_predicates = sorted(predicates_with_gt, key=lambda x: x[1]['recall'], reverse=True)
        
        print(f"\n{'='*80}")
        print(f"ğŸ“Š K={k} æ—¶çš„è°“è¯æ€§èƒ½è¯¦æƒ…")
        print(f"{'='*80}")
        
        print(f"\nğŸ“‹ æ‰€æœ‰è°“è¯æ€§èƒ½è¯¦æƒ… (å…± {len(sorted_predicates)} ä¸ªè°“è¯):")
        print(f"{'æ’å':<6}{'è°“è¯':<25}{'Recall':<12}{'å‘½ä¸­/æ€»æ•°':<15}")
        print("-"*60)
        for i, (pred, stats) in enumerate(sorted_predicates, 1):
            print(f"{i:<6}{pred:<25}{stats['recall']:<12.4f}{stats['hit']}/{stats['total']}")
    
    print()


def analyze_predicate_performance(data: Dict) -> None:
    """åˆ†ææ¯ä¸ªè°“è¯ç±»åˆ«çš„æ€§èƒ½"""
    print("="*80)
    print("ğŸ“Š è°“è¯ç±»åˆ«æ€§èƒ½åˆ†æ (åŸºäºé¢„æµ‹æ—¶çš„Kå€¼)")
    print("="*80)
    
    per_predicate = data.get('mean_recall_per_predicate', {})
    
    if not per_predicate:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°è°“è¯çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯\n")
        return
    
    # æŒ‰recallæ’åº
    sorted_predicates = sorted(
        per_predicate.items(),
        key=lambda x: x[1]['recall'],
        reverse=True
    )
    
    # åªæ˜¾ç¤ºæœ‰GTçš„è°“è¯
    predicates_with_gt = [(pred, stats) for pred, stats in sorted_predicates if stats['total'] > 0]
    
    if not predicates_with_gt:
        print("âš ï¸  æ²¡æœ‰æœ‰æ•ˆçš„è°“è¯ç»Ÿè®¡\n")
        return
    
    print(f"\næ€»å…± {len(predicates_with_gt)} ä¸ªè°“è¯ç±»åˆ«æœ‰GTæ•°æ®\n")
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    recalls = [stats['recall'] for _, stats in predicates_with_gt]
    mean_recall = np.mean(recalls)
    median_recall = np.median(recalls)
    std_recall = np.std(recalls)
    
    print(f"è°“è¯Recallç»Ÿè®¡:")
    print(f"  å¹³å‡å€¼: {mean_recall:.4f}")
    print(f"  ä¸­ä½æ•°: {median_recall:.4f}")
    print(f"  æ ‡å‡†å·®: {std_recall:.4f}")
    print(f"  æœ€å¤§å€¼: {max(recalls):.4f}")
    print(f"  æœ€å°å€¼: {min(recalls):.4f}")
    
    # æ˜¾ç¤ºæ‰€æœ‰è°“è¯ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
    print("\nğŸ“‹ æ‰€æœ‰è°“è¯æ€§èƒ½æ’åï¼ˆæŒ‰Recallæ’åºï¼‰:")
    print(f"{'æ’å':<6}{'è°“è¯':<25}{'Recall':<10}{'å‘½ä¸­/æ€»æ•°':<15}")
    print("-"*60)
    for i, (pred, stats) in enumerate(predicates_with_gt, 1):
        print(f"{i:<6}{pred:<25}{stats['recall']:<10.4f}{stats['hit']}/{stats['total']:<15}")
    
    print()


def analyze_image_performance(data: Dict) -> None:
    """åˆ†ææ¯å¼ å›¾ç‰‡çš„æ€§èƒ½åˆ†å¸ƒ"""
    print("="*80)
    print("ğŸ“Š å›¾ç‰‡çº§åˆ«æ€§èƒ½åˆ†æ")
    print("="*80)
    
    per_image_results = data.get('per_image_results', [])
    
    if not per_image_results:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯\n")
        return
    
    recalls = [img['recall@k'] for img in per_image_results]
    gt_relations = [img['total_gt_relations'] for img in per_image_results]
    recalled = [img['recalled_relations'] for img in per_image_results]
    
    print(f"\næ€»å›¾ç‰‡æ•°: {len(per_image_results)}")
    print(f"\nRecall@K åˆ†å¸ƒç»Ÿè®¡:")
    print(f"  å¹³å‡å€¼: {np.mean(recalls):.4f}")
    print(f"  ä¸­ä½æ•°: {np.median(recalls):.4f}")
    print(f"  æ ‡å‡†å·®: {np.std(recalls):.4f}")
    print(f"  æœ€å¤§å€¼: {np.max(recalls):.4f}")
    print(f"  æœ€å°å€¼: {np.min(recalls):.4f}")
    
    # ç™¾åˆ†ä½æ•°
    print(f"\nRecall@K ç™¾åˆ†ä½æ•°:")
    for percentile in [25, 50, 75, 90, 95]:
        value = np.percentile(recalls, percentile)
        print(f"  {percentile}th: {value:.4f}")
    
    # æŒ‰recallåˆ†ç»„ç»Ÿè®¡
    print(f"\nRecall@K åˆ†ç»„ç»Ÿè®¡:")
    bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
    for i in range(len(bins)-1):
        count = sum(1 for r in recalls if bins[i] <= r < bins[i+1])
        percentage = count / len(recalls) * 100
        print(f"  [{bins[i]:.1f}, {bins[i+1]:.1f}): {count:4d} å¼ å›¾ç‰‡ ({percentage:5.2f}%)")
    
    # æ˜¾ç¤ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„å›¾ç‰‡
    sorted_images = sorted(per_image_results, key=lambda x: x['recall@k'], reverse=True)
    
    print("\nğŸ† Top-20 è¡¨ç°æœ€å¥½çš„å›¾ç‰‡:")
    print(f"{'å›¾ç‰‡ID':<15}{'Recall@K':<12}{'å¬å›å…³ç³»':<15}{'æ€»å…³ç³»':<10}")
    print("-"*60)
    for img in sorted_images[:20]:
        print(f"{str(img['image_id']):<15}{img['recall@k']:<12.4f}"
              f"{img['recalled_relations']:<15}{img['total_gt_relations']:<10}")
    
    print("\nâš ï¸  Bottom-20 è¡¨ç°æœ€å·®çš„å›¾ç‰‡:")
    print(f"{'å›¾ç‰‡ID':<15}{'Recall@K':<12}{'å¬å›å…³ç³»':<15}{'æ€»å…³ç³»':<10}")
    print("-"*60)
    for img in sorted_images[-20:]:
        print(f"{str(img['image_id']):<15}{img['recall@k']:<12.4f}"
              f"{img['recalled_relations']:<15}{img['total_gt_relations']:<10}")
    
    print()


def analyze_relation_count_distribution(data: Dict) -> None:
    """åˆ†æå›¾åƒå…³ç³»æ•°é‡åˆ†å¸ƒ"""
    print("="*80)
    print("ğŸ“Š å›¾åƒå…³ç³»æ•°é‡åˆ†å¸ƒåˆ†æ")
    print("="*80)
    
    per_image_results = data.get('per_image_results', [])
    
    if not per_image_results:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯\n")
        return
    
    # æ”¶é›†æ‰€æœ‰å›¾ç‰‡çš„å…³ç³»æ•°é‡
    relation_counts = [img['total_gt_relations'] for img in per_image_results]
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“ˆ å…³ç³»æ•°é‡åŸºæœ¬ç»Ÿè®¡:")
    print(f"  æ€»å›¾ç‰‡æ•°: {len(relation_counts)}")
    print(f"  å¹³å‡å…³ç³»æ•°: {np.mean(relation_counts):.2f}")
    print(f"  ä¸­ä½æ•°å…³ç³»æ•°: {np.median(relation_counts):.2f}")
    print(f"  æ ‡å‡†å·®: {np.std(relation_counts):.2f}")
    print(f"  æœ€å°å€¼: {np.min(relation_counts)}")
    print(f"  æœ€å¤§å€¼: {np.max(relation_counts)}")
    
    # ç™¾åˆ†ä½æ•°
    print(f"\nğŸ“Š å…³ç³»æ•°é‡ç™¾åˆ†ä½æ•°:")
    for percentile in [25, 50, 75, 90, 95, 99]:
        value = np.percentile(relation_counts, percentile)
        print(f"  {percentile}th: {value:.1f}")
    
    # åˆ†å¸ƒç»Ÿè®¡
    print(f"\nğŸ“Š å…³ç³»æ•°é‡åˆ†å¸ƒ:")
    bins = [0, 5, 10, 15, 20, 25, 30, 50, 100, float('inf')]
    bin_labels = ['1-5', '6-10', '11-15', '16-20', '21-25', '26-30', '31-50', '51-100', '100+']
    
    for i in range(len(bins)-1):
        count = sum(1 for c in relation_counts if bins[i] < c <= bins[i+1])
        percentage = count / len(relation_counts) * 100
        print(f"  {bin_labels[i]:<8}: {count:4d} å¼ å›¾ç‰‡ ({percentage:5.2f}%)")
    
    # å…³ç³»æ•°é‡é¢‘ç‡è¡¨
    from collections import Counter
    relation_freq = Counter(relation_counts)
    print(f"\nğŸ“Š å…·ä½“å…³ç³»æ•°é‡é¢‘ç‡ (å…¨éƒ¨):")
    print(f"{'å…³ç³»æ•°':<8}{'å›¾ç‰‡æ•°':<8}{'ç™¾åˆ†æ¯”':<10}")
    print("-"*30)
    for num_rel, count in relation_freq.most_common():
        percentage = count / len(relation_counts) * 100
        print(f"{num_rel:<8}{count:<8}{percentage:<10.2f}")
    
    print()


def analyze_relation_count_impact(data: Dict) -> None:
    """åˆ†æå›¾ç‰‡ä¸­å…³ç³»æ•°é‡å¯¹Recallçš„å½±å“"""
    print("="*80)
    print("ğŸ“Š å…³ç³»æ•°é‡å¯¹Recallçš„å½±å“åˆ†æ")
    print("="*80)
    
    per_image_results = data.get('per_image_results', [])
    
    if not per_image_results:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯\n")
        return
    
    # æŒ‰å…³ç³»æ•°é‡åˆ†ç»„
    relation_groups = defaultdict(list)
    for img in per_image_results:
        num_relations = img['total_gt_relations']
        relation_groups[num_relations].append(img['recall@k'])
    
    # è®¡ç®—æ¯ç»„çš„å¹³å‡recall
    group_stats = []
    for num_rel, recalls in sorted(relation_groups.items()):
        avg_recall = np.mean(recalls)
        median_recall = np.median(recalls)
        group_stats.append({
            'num_relations': num_rel,
            'avg_recall': avg_recall,
            'median_recall': median_recall,
            'num_images': len(recalls),
            'std_recall': np.std(recalls),
            'min_recall': np.min(recalls),
            'max_recall': np.max(recalls)
        })
    
    print(f"\nğŸ“ˆ æŒ‰å…³ç³»æ•°é‡åˆ†ç»„çš„Recallæ€§èƒ½:")
    print(f"{'å…³ç³»æ•°':<8}{'å›¾ç‰‡æ•°':<8}{'å¹³å‡Recall':<12}{'ä¸­ä½æ•°Recall':<15}{'æ ‡å‡†å·®':<10}{'èŒƒå›´':<15}")
    print("-"*80)
    for stat in group_stats:
        recall_range = f"{stat['min_recall']:.3f}-{stat['max_recall']:.3f}"
        print(f"{stat['num_relations']:<8}{stat['num_images']:<8}"
              f"{stat['avg_recall']:<12.4f}{stat['median_recall']:<15.4f}"
              f"{stat['std_recall']:<10.4f}{recall_range:<15}")
    
    # æŒ‰å…³ç³»æ•°é‡åŒºé—´åˆ†æ
    print(f"\nğŸ“Š æŒ‰å…³ç³»æ•°é‡åŒºé—´çš„æ€§èƒ½åˆ†æ:")
    relation_ranges = [
        (1, 5, "1-5ä¸ªå…³ç³»"),
        (6, 10, "6-10ä¸ªå…³ç³»"), 
        (11, 15, "11-15ä¸ªå…³ç³»"),
        (16, 20, "16-20ä¸ªå…³ç³»"),
        (21, 30, "21-30ä¸ªå…³ç³»"),
        (31, 50, "31-50ä¸ªå…³ç³»"),
        (51, 100, "51-100ä¸ªå…³ç³»"),
        (101, float('inf'), "100+ä¸ªå…³ç³»")
    ]
    
    print(f"{'å…³ç³»æ•°èŒƒå›´':<15}{'å›¾ç‰‡æ•°':<8}{'å¹³å‡Recall':<12}{'ä¸­ä½æ•°Recall':<15}{'æ ‡å‡†å·®':<10}")
    print("-"*70)
    
    for min_rel, max_rel, label in relation_ranges:
        range_images = [img for img in per_image_results 
                       if min_rel <= img['total_gt_relations'] <= max_rel]
        
        if len(range_images) > 0:
            recalls = [img['recall@k'] for img in range_images]
            avg_recall = np.mean(recalls)
            median_recall = np.median(recalls)
            std_recall = np.std(recalls)
            
            print(f"{label:<15}{len(range_images):<8}{avg_recall:<12.4f}"
                  f"{median_recall:<15.4f}{std_recall:<10.4f}")
    
    # ç›¸å…³æ€§åˆ†æ
    relation_counts = [img['total_gt_relations'] for img in per_image_results]
    recalls = [img['recall@k'] for img in per_image_results]
    
    correlation = np.corrcoef(relation_counts, recalls)[0, 1]
    print(f"\nğŸ“ˆ å…³ç³»æ•°é‡ä¸Recallçš„ç›¸å…³æ€§:")
    print(f"  çš®å°”é€Šç›¸å…³ç³»æ•°: {correlation:.4f}")
    
    if correlation > 0.1:
        print("  â†’ å…³ç³»æ•°é‡ä¸Recallå‘ˆæ­£ç›¸å…³")
    elif correlation < -0.1:
        print("  â†’ å…³ç³»æ•°é‡ä¸Recallå‘ˆè´Ÿç›¸å…³")
    else:
        print("  â†’ å…³ç³»æ•°é‡ä¸Recallç›¸å…³æ€§è¾ƒå¼±")
    
    print()


def analyze_detailed_relation_performance(data: Dict) -> None:
    """è¯¦ç»†åˆ†æä¸åŒå…³ç³»æ•°é‡ä¸‹çš„æ€§èƒ½è¡¨ç°"""
    print("="*80)
    print("ğŸ“Š è¯¦ç»†å…³ç³»æ•°é‡æ€§èƒ½åˆ†æ")
    print("="*80)
    
    per_image_results = data.get('per_image_results', [])
    
    if not per_image_results:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°å›¾ç‰‡çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯\n")
        return
    
    # æŒ‰å…³ç³»æ•°é‡åˆ†ç»„ï¼Œæ›´ç»†è‡´çš„åˆ†æ
    relation_groups = defaultdict(list)
    for img in per_image_results:
        num_relations = img['total_gt_relations']
        relation_groups[num_relations].append({
            'image_id': img.get('image_id', 'unknown'),
            'recall': img['recall@k'],
            'recalled_relations': img.get('recalled_relations', 0),
            'total_gt_relations': img['total_gt_relations']
        })
    
    # åˆ†ææ¯ä¸ªå…³ç³»æ•°é‡ä¸‹çš„æ€§èƒ½
    print(f"\nğŸ“ˆ å„å…³ç³»æ•°é‡ä¸‹çš„è¯¦ç»†æ€§èƒ½:")
    print(f"{'å…³ç³»æ•°':<8}{'å›¾ç‰‡æ•°':<8}{'å¹³å‡Recall':<12}{'ä¸­ä½æ•°Recall':<15}{'æ ‡å‡†å·®':<10}{'æœ€ä½³Recall':<12}{'æœ€å·®Recall':<12}")
    print("-"*90)
    
    for num_rel in sorted(relation_groups.keys()):
        group_data = relation_groups[num_rel]
        recalls = [item['recall'] for item in group_data]
        
        avg_recall = np.mean(recalls)
        median_recall = np.median(recalls)
        std_recall = np.std(recalls)
        best_recall = np.max(recalls)
        worst_recall = np.min(recalls)
        
        print(f"{num_rel:<8}{len(group_data):<8}{avg_recall:<12.4f}"
              f"{median_recall:<15.4f}{std_recall:<10.4f}"
              f"{best_recall:<12.4f}{worst_recall:<12.4f}")
    
    # æŒ‰å…³ç³»æ•°é‡åŒºé—´è¿›è¡Œæ›´ç»†è‡´çš„åˆ†æ
    print(f"\nğŸ“Š æŒ‰å…³ç³»æ•°é‡åŒºé—´çš„è¯¦ç»†åˆ†æ:")
    
    # å®šä¹‰æ›´ç»†è‡´çš„åŒºé—´
    detailed_ranges = [
        (1, 3, "1-3ä¸ªå…³ç³»"),
        (4, 6, "4-6ä¸ªå…³ç³»"),
        (7, 10, "7-10ä¸ªå…³ç³»"),
        (11, 15, "11-15ä¸ªå…³ç³»"),
        (16, 20, "16-20ä¸ªå…³ç³»"),
        (21, 25, "21-25ä¸ªå…³ç³»"),
        (26, 30, "26-30ä¸ªå…³ç³»"),
        (31, 40, "31-40ä¸ªå…³ç³»"),
        (41, 50, "41-50ä¸ªå…³ç³»"),
        (51, 75, "51-75ä¸ªå…³ç³»"),
        (76, 100, "76-100ä¸ªå…³ç³»"),
        (101, float('inf'), "100+ä¸ªå…³ç³»")
    ]
    
    print(f"{'å…³ç³»æ•°èŒƒå›´':<15}{'å›¾ç‰‡æ•°':<8}{'å¹³å‡Recall':<12}{'ä¸­ä½æ•°Recall':<15}{'æ ‡å‡†å·®':<10}{'æœ€ä½³':<8}{'æœ€å·®':<8}")
    print("-"*90)
    
    for min_rel, max_rel, label in detailed_ranges:
        range_images = [img for img in per_image_results 
                       if min_rel <= img['total_gt_relations'] <= max_rel]
        
        if len(range_images) > 0:
            recalls = [img['recall@k'] for img in range_images]
            avg_recall = np.mean(recalls)
            median_recall = np.median(recalls)
            std_recall = np.std(recalls)
            best_recall = np.max(recalls)
            worst_recall = np.min(recalls)
            
            print(f"{label:<15}{len(range_images):<8}{avg_recall:<12.4f}"
                  f"{median_recall:<15.4f}{std_recall:<10.4f}"
                  f"{best_recall:<8.4f}{worst_recall:<8.4f}")
    
    # æ‰¾å‡ºæ€§èƒ½æœ€å¥½å’Œæœ€å·®çš„å…³ç³»æ•°é‡
    print(f"\nğŸ† æ€§èƒ½åˆ†ææ€»ç»“:")
    
    # è®¡ç®—æ¯ä¸ªå…³ç³»æ•°é‡çš„å¹³å‡recall
    relation_performance = {}
    for num_rel, group_data in relation_groups.items():
        recalls = [item['recall'] for item in group_data]
        relation_performance[num_rel] = {
            'avg_recall': np.mean(recalls),
            'count': len(recalls),
            'std': np.std(recalls)
        }
    
    # æŒ‰å¹³å‡recallæ’åº
    sorted_performance = sorted(relation_performance.items(), 
                              key=lambda x: x[1]['avg_recall'], reverse=True)
    
    print(f"\nğŸ“ˆ å…³ç³»æ•°é‡æ€§èƒ½æ’å (å…¨éƒ¨):")
    print(f"{'æ’å':<6}{'å…³ç³»æ•°':<8}{'å›¾ç‰‡æ•°':<8}{'å¹³å‡Recall':<12}{'æ ‡å‡†å·®':<10}")
    print("-"*50)
    for i, (num_rel, stats) in enumerate(sorted_performance, 1):
        print(f"{i:<6}{num_rel:<8}{stats['count']:<8}"
              f"{stats['avg_recall']:<12.4f}{stats['std']:<10.4f}")
    
    # åˆ†ææ€§èƒ½è¶‹åŠ¿
    print(f"\nğŸ“Š æ€§èƒ½è¶‹åŠ¿åˆ†æ:")
    if len(sorted_performance) >= 3:
        best_relation_count = sorted_performance[0][0]
        worst_relation_count = sorted_performance[-1][0]
        
        print(f"  æœ€ä½³æ€§èƒ½å…³ç³»æ•°: {best_relation_count} (Recall: {sorted_performance[0][1]['avg_recall']:.4f})")
        print(f"  æœ€å·®æ€§èƒ½å…³ç³»æ•°: {worst_relation_count} (Recall: {sorted_performance[-1][1]['avg_recall']:.4f})")
        
        # åˆ†ææ˜¯å¦å­˜åœ¨æ˜æ˜¾è¶‹åŠ¿
        relation_counts = [item[0] for item in sorted_performance]
        avg_recalls = [item[1]['avg_recall'] for item in sorted_performance]
        
        # è®¡ç®—å…³ç³»æ•°é‡ä¸recallçš„ç›¸å…³æ€§
        correlation = np.corrcoef(relation_counts, avg_recalls)[0, 1]
        print(f"  å…³ç³»æ•°é‡ä¸æ€§èƒ½ç›¸å…³æ€§: {correlation:.4f}")
        
        if correlation > 0.3:
            print("  â†’ å…³ç³»æ•°é‡è¶Šå¤šï¼Œæ€§èƒ½è¶Šå¥½")
        elif correlation < -0.3:
            print("  â†’ å…³ç³»æ•°é‡è¶Šå¤šï¼Œæ€§èƒ½è¶Šå·®")
        else:
            print("  â†’ å…³ç³»æ•°é‡ä¸æ€§èƒ½æ²¡æœ‰æ˜æ˜¾çº¿æ€§å…³ç³»")
    
    print()


def print_summary(data: Dict) -> None:
    """æ‰“å°æ€»ç»“ä¿¡æ¯"""
    print("="*80)
    print("ğŸ“‹ æ€»ç»“æŠ¥å‘Š")
    print("="*80)
    
    summary = data.get('summary', {})
    
    if not summary:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°æ€»ç»“ä¿¡æ¯\n")
        return
    
    print(f"\nè¯„ä¼°æ–¹æ³•: {summary.get('evaluation_method', 'N/A')}")
    print(f"æ€»å›¾ç‰‡æ•°: {summary.get('total_images', 0)}")
    print(f"æ€»å…³ç³»æ•°: {summary.get('total_relations', 0)}")
    print(f"æ€»å€™é€‰æ•°: {summary.get('total_candidates', 0)}")
    
    print(f"\nä¸»è¦æŒ‡æ ‡:")
    print(f"  Average Recall@50: {summary.get('avg_recall@50', 0):.4f} ({summary.get('avg_recall@50', 0)*100:.2f}%)")
    print(f"  Mean Recall@50:    {summary.get('mean_recall@50', 0):.4f} ({summary.get('mean_recall@50', 0)*100:.2f}%)")
    
    print(f"\nå¬å›ç»Ÿè®¡:")
    print(f"  æ€»å¬å›å…³ç³»æ•°: {summary.get('total_recalled_relations', 0)}")
    print(f"  æ€»GTå…³ç³»æ•°:   {summary.get('total_gt_relations', 0)}")
    if summary.get('total_gt_relations', 0) > 0:
        overall_recall = summary.get('total_recalled_relations', 0) / summary.get('total_gt_relations', 1)
        print(f"  æ•´ä½“å¬å›ç‡:   {overall_recall:.4f} ({overall_recall*100:.2f}%)")
    
    print(f"\nè°“è¯ç»Ÿè®¡:")
    print(f"  æœ‰æ•ˆè°“è¯ç±»åˆ«æ•°: {summary.get('num_valid_predicates', 0)}")
    
    if summary.get('images_with_insufficient_candidates', 0) > 0:
        print(f"\nâš ï¸  æ³¨æ„:")
        print(f"  æœ‰ {summary.get('images_with_insufficient_candidates', 0)} å¼ å›¾ç‰‡çš„å€™é€‰æ•°ä¸è¶³50")
    
    print()


def export_detailed_report(data: Dict, output_path: str, additional_metrics: Dict = None) -> None:
    """å¯¼å‡ºè¯¦ç»†çš„è¯„ä¼°æŠ¥å‘Š"""
    print(f"ğŸ’¾ æ­£åœ¨å¯¼å‡ºè¯¦ç»†æŠ¥å‘Šåˆ°: {output_path}")
    
    report = {
        'summary': data.get('summary', {}),
        'additional_metrics': additional_metrics or {},
        'per_image_statistics': {
            'total_images': len(data.get('per_image_results', [])),
            'recall_distribution': {}
        },
        'per_predicate_statistics': {
            'total_predicates': len([p for p, s in data.get('mean_recall_per_predicate', {}).items() if s['total'] > 0]),
        }
    }
    
    # æ·»åŠ å›¾ç‰‡çº§åˆ«ç»Ÿè®¡
    if data.get('per_image_results'):
        recalls = [img['recall@k'] for img in data['per_image_results']]
        report['per_image_statistics']['recall_distribution'] = {
            'mean': float(np.mean(recalls)),
            'median': float(np.median(recalls)),
            'std': float(np.std(recalls)),
            'min': float(np.min(recalls)),
            'max': float(np.max(recalls)),
            'percentiles': {
                '25th': float(np.percentile(recalls, 25)),
                '50th': float(np.percentile(recalls, 50)),
                '75th': float(np.percentile(recalls, 75)),
                '90th': float(np.percentile(recalls, 90)),
                '95th': float(np.percentile(recalls, 95)),
            }
        }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æŠ¥å‘Šå·²å¯¼å‡º\n")


def main():
    parser = argparse.ArgumentParser(
        description="è¯„ä¼°åœºæ™¯å›¾å…³ç³»é¢„æµ‹ç»“æœ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬è¯„ä¼°
  python evaluate_results.py results.json
  
  # åŒ…å«Mean Rankè®¡ç®—ï¼ˆéœ€è¦å®Œæ•´å€™é€‰åˆ—è¡¨ï¼‰
  python evaluate_results.py results.json --calculate-mr
  
  # è®¡ç®—å¤šä¸ªKå€¼çš„Recall
  python evaluate_results.py results.json --multi-k
  
  # è®¡ç®—å¤šä¸ªKå€¼ä¸‹çš„è°“è¯çº§åˆ«Mean Recall
  python evaluate_results.py results.json --predicate-multi-k
  
  # è®¡ç®—Baseå’ŒNovelè°“è¯ç±»åˆ«çš„Recall
  python evaluate_results.py results.json --category-recall
  
  # è‡ªå®šä¹‰Kå€¼åˆ—è¡¨
  python evaluate_results.py results.json --multi-k --predicate-multi-k --category-recall --k-values 1 5 10 20 50
  
  # å¯¼å‡ºè¯¦ç»†æŠ¥å‘Š
  python evaluate_results.py results.json --export report.json
  
  # å®Œæ•´åˆ†æ
  python evaluate_results.py results.json --calculate-mr --multi-k --predicate-multi-k --category-recall --export report.json
        """
    )
    
    parser.add_argument('--json_file', type=str, default='/public/home/xiaojw2025/Workspace/VLM2Vec/predict/recall_results_2000_qwen2vl_2b_instruct.json', help='é¢„æµ‹ç»“æœJSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--calculate-mr', action='store_true', 
                       help='è®¡ç®—Mean Rank (MR) æŒ‡æ ‡ï¼ˆéœ€è¦å®Œæ•´å€™é€‰åˆ—è¡¨ï¼‰')
    parser.add_argument('--multi-k', action='store_true',
                       help='è®¡ç®—å¤šä¸ªKå€¼ä¸‹çš„Recall@Kï¼ˆéœ€è¦å®Œæ•´å€™é€‰åˆ—è¡¨ï¼‰')
    parser.add_argument('--predicate-multi-k', action='store_true',
                       help='è®¡ç®—å¤šä¸ªKå€¼ä¸‹çš„è°“è¯çº§åˆ«Mean Recallï¼ˆéœ€è¦å®Œæ•´å€™é€‰åˆ—è¡¨ï¼‰')
    parser.add_argument('--category-recall', action='store_true',
                       help='è®¡ç®—Baseå’ŒNovelè°“è¯ç±»åˆ«çš„Recall@Kï¼ˆéœ€è¦å®Œæ•´å€™é€‰åˆ—è¡¨ï¼‰')
    parser.add_argument('--k-values', type=int, nargs='+', default=[50, 100],
                       help='æŒ‡å®šè¦è®¡ç®—çš„Kå€¼åˆ—è¡¨ï¼ˆé»˜è®¤: 1 5 10 20 50 100ï¼‰')
    parser.add_argument('--export', type=str, default=None,
                       help='å¯¼å‡ºè¯¦ç»†æŠ¥å‘Šåˆ°æŒ‡å®šJSONæ–‡ä»¶')
    parser.add_argument('--no-predicate-analysis', action='store_true',
                       help='è·³è¿‡è°“è¯çº§åˆ«åˆ†æ')
    parser.add_argument('--no-image-analysis', action='store_true',
                       help='è·³è¿‡å›¾ç‰‡çº§åˆ«åˆ†æ')
    parser.add_argument('--detailed-relation-analysis', action='store_true',
                       help='è¿›è¡Œè¯¦ç»†çš„å…³ç³»æ•°é‡æ€§èƒ½åˆ†æ')
    parser.add_argument('--no-relation-distribution', action='store_true',
                       help='è·³è¿‡å…³ç³»æ•°é‡åˆ†å¸ƒåˆ†æ')
    
    args = parser.parse_args()
    
    # åŠ è½½ç»“æœ
    data = load_results(args.json_file)
    
    # æ‰“å°æ€»ç»“
    print_summary(data)
    
    # é¢å¤–çš„æŒ‡æ ‡
    additional_metrics = {}
    
    # è®¡ç®—Mean Rankï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    mr_results = calculate_mean_rank(data)
    if mr_results:
        additional_metrics['mean_rank_metrics'] = mr_results
    
    # è®¡ç®—å¤šä¸ªKå€¼çš„Recallï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    multi_k_results = calculate_recall_at_multiple_k(data, args.k_values)
    if multi_k_results:
        additional_metrics['multi_k_recall'] = multi_k_results
    
    # è®¡ç®—å¤šä¸ªKå€¼ä¸‹çš„è°“è¯çº§åˆ«Mean Recallï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    predicate_multi_k_results = calculate_mean_recall_per_predicate_multi_k(data, args.k_values)
    if predicate_multi_k_results:
        additional_metrics['predicate_mean_recall_multi_k'] = predicate_multi_k_results
    
    # è®¡ç®—Baseå’ŒNovelè°“è¯ç±»åˆ«çš„Recallï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    category_recall_results = calculate_category_recall_at_k(data, args.k_values)
    if category_recall_results:
        additional_metrics['category_recall'] = category_recall_results
    
    # è°“è¯çº§åˆ«åˆ†æï¼ˆé»˜è®¤å¯ç”¨ï¼Œé™¤éæ˜ç¡®ç¦ç”¨ï¼‰
    if not args.no_predicate_analysis:
        analyze_predicate_performance(data)
    
    # å±•ç¤ºå¤šKå€¼ä¸‹çš„è°“è¯MRè¯¦ç»†ç»“æœï¼ˆæ˜¾ç¤ºå…¨éƒ¨ï¼Œæ‰€æœ‰Kå€¼ï¼‰
    if predicate_multi_k_results:
        display_predicate_multi_k_results(predicate_multi_k_results, top_n=9999, detail_k_values=args.k_values)
    
    # å±•ç¤ºBaseå’ŒNovelè°“è¯ç±»åˆ«çš„Recallç»“æœ
    if category_recall_results:
        display_category_recall_results(category_recall_results)
    
    # å›¾ç‰‡çº§åˆ«åˆ†æï¼ˆé»˜è®¤å¯ç”¨ï¼Œé™¤éæ˜ç¡®ç¦ç”¨ï¼‰
    if not args.no_image_analysis:
        analyze_image_performance(data)
    
    # å…³ç³»æ•°é‡åˆ†å¸ƒåˆ†æï¼ˆé»˜è®¤å¯ç”¨ï¼Œé™¤éæ˜ç¡®ç¦ç”¨ï¼‰
    if not args.no_relation_distribution:
        analyze_relation_count_distribution(data)
    
    # å…³ç³»æ•°é‡å½±å“åˆ†æï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    analyze_relation_count_impact(data)
    
    # è¯¦ç»†å…³ç³»æ•°é‡æ€§èƒ½åˆ†æï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    analyze_detailed_relation_performance(data)
    
    # å¯¼å‡ºæŠ¥å‘Š
    if args.export:
        export_detailed_report(data, args.export, additional_metrics)
    
    print("="*80)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("="*80)


if __name__ == "__main__":
    main()

